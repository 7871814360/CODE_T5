{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Question: Program to find LCM?\n",
      "Matched Question: 55 write a  program to find LCM\n",
      "\n",
      "Retrieved Solution: \n",
      "def lcm(x, y):  \n",
      "   if x > y:  \n",
      "       greater = x  \n",
      "   else:  \n",
      "       greater = y  \n",
      "   while(True):  \n",
      "       if((greater % x == 0) and (greater % y == 0)):  \n",
      "           lcm = greater  \n",
      "           break  \n",
      "       greater += 1  \n",
      "   return lcm  \n",
      "  \n",
      "  \n",
      "num1 = int(input(\"Enter first number: \"))  \n",
      "num2 = int(input(\"Enter second number: \"))  \n",
      "print(\"The L.C.M. of\", num1,\"and\", num2,\"is\", lcm(num1, num2)) \n",
      "\n",
      "\n",
      "Generated Code:\n",
      "# Output Question: Program to find LCM?\n",
      "# Retrieved Solution: \n",
      "def lcm(x, y):  \n",
      "   if x > y:  \n",
      "       greater = x  \n",
      "   else:  \n",
      "       greater = y  \n",
      "   while(True):  \n",
      "       if((greater % x == 0) and (greater % y == 0)):  \n",
      "           lcm = greater  \n",
      "           break  \n",
      "       greater += 1  \n",
      "   return lcm  \n",
      "  \n",
      "num1 = int(input(\"Enter first number: \"))  \n",
      "num2 = int(input(\"Enter second number: \"))  \n",
      "print(\"The L.C.M. of\", num1,\"and\", num2,\"is\", lcm(num1, num2))  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('python_code.csv')\n",
    "\n",
    "# Load the question and solution columns\n",
    "questions = data['question'].tolist()\n",
    "solutions = data['solution'].tolist()\n",
    "\n",
    "# Step 1: Encode the questions into embeddings using a sentence transformer\n",
    "embedder_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "question_embeddings = embedder_model.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# Step 2: Create FAISS index for retrieval\n",
    "d = question_embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance-based index\n",
    "index.add(np.array(question_embeddings.cpu()))\n",
    "\n",
    "# Set a threshold for L2 distance (you can adjust this based on your data)\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Function to retrieve the nearest question based on input\n",
    "def retrieve_answer(input_question):\n",
    "    # Encode the input question\n",
    "    input_embedding = embedder_model.encode([input_question], convert_to_tensor=True)\n",
    "    input_embedding = np.array(input_embedding.cpu())\n",
    "    \n",
    "    # Retrieve the nearest question\n",
    "    D, I = index.search(input_embedding, k=1)  # k=1 for the top match\n",
    "    distance = D[0][0]\n",
    "    \n",
    "    # Check if the distance is below the similarity threshold\n",
    "    if distance < SIMILARITY_THRESHOLD:\n",
    "        return solutions[I[0][0]], questions[I[0][0]]  # Return solution and matched question\n",
    "    else:\n",
    "        return None, None  # Return None if no close match is found\n",
    "\n",
    "# Step 3: Load CodeT5 for code generation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-large-ntp-py\")\n",
    "codet5_model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-large-ntp-py\")\n",
    "\n",
    "# Function to generate code using CodeT5\n",
    "def generate_code(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    generated_tokens = codet5_model.generate(**inputs, max_length=200)\n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 4: Combine Retrieval and Generation (RAG)\n",
    "def rag_generate(input_question):\n",
    "    # Step 4.1: Retrieve the most similar question and solution\n",
    "    retrieved_solution, matched_question = retrieve_answer(input_question)\n",
    "\n",
    "    if retrieved_solution is not None:\n",
    "        # Step 4.2: Use the retrieved solution as part of the prompt to generate refined code\n",
    "        prompt = f\"# Input Question: {input_question}\\n# Retrieved Solution: {retrieved_solution}\\n\"\n",
    "        generated_code = generate_code(prompt)\n",
    "        \n",
    "        return {\n",
    "            'input_question': input_question,\n",
    "            'matched_question': matched_question,\n",
    "            'retrieved_solution': retrieved_solution,\n",
    "            'generated_code': generated_code\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'input_question': input_question,\n",
    "            'error': \"No relevant questions found.\"\n",
    "        }\n",
    "\n",
    "# Test with a new question\n",
    "new_question = \"Program to find LCM?\"\n",
    "result = rag_generate(new_question)\n",
    "\n",
    "if 'error' not in result:\n",
    "    print(f\"Input Question: {result['input_question']}\")\n",
    "    print(f\"Matched Question: {result['matched_question']}\")\n",
    "    print(f\"Retrieved Solution: {result['retrieved_solution']}\")\n",
    "    print(f\"Generated Code:\\n{result['generated_code']}\")\n",
    "else:\n",
    "    print(result['error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check if GPU (CUDA) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('python_code.csv')\n",
    "\n",
    "# Load the question and solution columns\n",
    "questions = data['question'].tolist()\n",
    "solutions = data['solution'].tolist()\n",
    "\n",
    "# Step 1: Encode the questions into embeddings using a sentence transformer\n",
    "embedder_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "question_embeddings = embedder_model.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# Step 2: Create FAISS index for retrieval\n",
    "d = question_embeddings.shape[1]  # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance-based index\n",
    "index.add(np.array(question_embeddings.cpu()))\n",
    "\n",
    "# Set a threshold for L2 distance (you can adjust this based on your data)\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "\n",
    "# Function to retrieve the nearest question based on input\n",
    "def retrieve_answer(input_question):\n",
    "    # Encode the input question\n",
    "    input_embedding = embedder_model.encode([input_question], convert_to_tensor=True)\n",
    "    input_embedding = np.array(input_embedding.cpu())\n",
    "    \n",
    "    # Retrieve the nearest question\n",
    "    D, I = index.search(input_embedding, k=1)  # k=1 for the top match\n",
    "    distance = D[0][0]\n",
    "    \n",
    "    # Check if the distance is below the similarity threshold\n",
    "    if distance < SIMILARITY_THRESHOLD:\n",
    "        return solutions[I[0][0]], questions[I[0][0]]  # Return solution and matched question\n",
    "    else:\n",
    "        return None, None  # Return None if no close match is found\n",
    "\n",
    "# Step 3: Load the quantized CodeT5 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-large-ntp-py\")\n",
    "codet5_model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-large-ntp-py\")\n",
    "\n",
    "# Apply dynamic quantization to reduce memory usage\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    codet5_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ").to(device)  # Move quantized model to GPU if available\n",
    "\n",
    "# Function to generate code using the quantized CodeT5 model\n",
    "def generate_code(prompt):\n",
    "    # Tokenize input prompt and move to the correct device (GPU or CPU)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate tokens\n",
    "    generated_tokens = quantized_model.generate(**inputs, max_length=200)\n",
    "    \n",
    "    # Decode generated tokens and return code\n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Step 4: Combine Retrieval and Generation (RAG)\n",
    "def rag_generate(input_question):\n",
    "    # Step 4.1: Retrieve the most similar question and solution\n",
    "    retrieved_solution, matched_question = retrieve_answer(input_question)\n",
    "\n",
    "    if retrieved_solution is not None:\n",
    "        # Step 4.2: Use the retrieved solution as part of the prompt to generate refined code\n",
    "        prompt = f\"# Input Question: {input_question}\\n# Retrieved Solution: {retrieved_solution}\\n\"\n",
    "        generated_code = generate_code(prompt)\n",
    "        \n",
    "        return {\n",
    "            'input_question': input_question,\n",
    "            'matched_question': matched_question,\n",
    "            'retrieved_solution': retrieved_solution,\n",
    "            'generated_code': generated_code\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'input_question': input_question,\n",
    "            'error': \"No relevant questions found.\"\n",
    "        }\n",
    "\n",
    "# Test with a new question\n",
    "new_question = \"Program to find LCM?\"\n",
    "result = rag_generate(new_question)\n",
    "\n",
    "# Print the results\n",
    "if 'error' not in result:\n",
    "    print(f\"Input Question: {result['input_question']}\")\n",
    "    print(f\"Matched Question: {result['matched_question']}\")\n",
    "    print(f\"Retrieved Solution: {result['retrieved_solution']}\")\n",
    "    print(f\"Generated Code:\\n{result['generated_code']}\")\n",
    "else:\n",
    "    print(result['error'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
